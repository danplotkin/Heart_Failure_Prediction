---
title: "Heart Failure Prediction"
author: "Daniel Plotkin"
date: "2022-11-10"
output: rmdformats::readthedown
---

# Introduction

Cardiovascular diseases (CVDs) are the **leading** **cause of death globally**. CVDs account for **31% of all deaths worldwide (17 million deaths per year)**. Heart failure is a common event that is caused by CVDs.

In this project, I will be focusing on predicting a classification of heart failure mortality based on 12 different predictor variables.

# Prerequisites

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)
library(tidymodels)
library(vip)
library(ggplot2)
library(corrplot)
library(pdp)
library(DALEXtra)
```

# Data Exploration

Below we will be importing our data set, describe our data, and explore relationships/distributions.

## Data Import

```{r, message=F}
# Data Import
path <- here('data', 'heart_failure_clinical_records_dataset.csv')
df <- read_csv(path)

DT::datatable(head(df))
str(df)

```

## About the Data

Predictor Variables ($X$)

-   ***age***: How many years old is the subject.

-   ***anaemia***: Whether or not the subject has anaemia (0 = yes, 1 = no).

-   ***creatinine_phosphokinase***: Level of the CPK enzyme in the blood (mcg/L).

-   ***diabetes***: If the patient has diabetes: (0 = no, 1 = yes).

-   ***ejection_fraction***: Percentage of blood leaving the heart at each contraction (percentage).

-   ***high_blood_pressure***: If the subject has high_blood_pressure (0 = no, 1 = yes).

-   ***palates***: Platelets in the blood (kiloplatelets/mL).

-   ***serum_creatinine***: Level of serum creatinine in the blood (mg/dL).

-   ***serum_sodium***: Level of serum sodium in the blood (mEq/L).

-   ***sex***: Gender of the subject (0 = female, 1 = male).

-   ***smoking***: If the subject smokes or not (0 = no, 1 = yes).

-   ***time***: Follow-up period (days).

Response Variable ($Y$)

-   ***DEATH_EVENT***: If the patient deceased during the follow-up period.

## Data Summary

```{r}
summary(df)
```

## Correlation Matrix

```{r}
corrplot(cor(df), tl.cex = 1)
```

We can derive a few insights from this correlation matrix:

-   There is a strong negative correlation between DEATH_EVENT and the time in between appointments.

-   There is a correlation between whether or not they are males and if they smoke or not. More males smoke than females.

## Data Exploration

```{r}
# function to create distribution charts
create_binary_distribution <- function(var, title = none, x = none){
  ggplot(df, aes(x = as.factor(var))) +
  geom_bar(aes(fill = as.factor(DEATH_EVENT))) +
  labs(
    title = title,
    x = x,
    fill = 'DEATH_EVENT'
  ) +
  scale_fill_manual(values = c('darkgreen', 'brown'))
}
```

```{r}
create_binary_distribution(
  var = df$DEATH_EVENT,
  title = 'Total Death Distribution',
  x = 'DEATH_EVENT'
)
```

There were more subjects that survived than died in our sample population.

```{r}
create_binary_distribution(
  var = df$sex,
  x = 'Gender',
  title = 'Total Gender Distribution'
)
```

There were more males sampled than females, but both genders had similar death to survival ratios.

```{r}
create_binary_distribution(
  var = df$anaemia,
  x = 'anaemia',
  title = 'Total Anaemia Distribution'
)
```

Anaemia did not play a big part in whether or not someone survived.

```{r}
create_binary_distribution(
  var = df$diabetes,
  x = 'diabetes',
  title = 'Total Diabetes Distribution'
)
```

Diabetes did not play a large role in whether or not someone experienced heart failure.

```{r}
create_binary_distribution(
  var = df$high_blood_pressure,
  x = 'high_blood_pressure',
  title = 'High Blood Pressure Distribution'
  )
```

High blood pressure did not play a large role in whether or not someone experienced heart failure.

```{r}
create_binary_distribution(
  var = df$smoking,
  x = 'smoking',
  title = 'Total Smoking Distribution'
)
```

Smoking did not play a large role in whether or not someone experienced heart failure.

```{r, message=FALSE}
df %>%
  ggplot(aes(age)) +
  geom_histogram(aes(fill = as.factor(DEATH_EVENT))) +
  scale_x_continuous(breaks = seq(30, 100, 10)) +
  labs(
    title = 'Age Distribution',
    fill = 'DEATH_EVENT',
  ) +
  theme(
  plot.title = element_text(hjust = 0.5)
) +
  scale_fill_manual(values = c('darkgreen', 'brown'))
```

While the age of our subjects ranged from 40-95, the mean age sampled was about 61. The age group of 60 represents the age group where most deaths occurred (13).

Now we are going to look at our numeric features to understand their effect on the response variable a bit better.

```{r}
lst = list(
  ejection_fraction = df$ejection_fraction,
  creatinine_phosphokinase = df$creatinine_phosphokinase,
  serum_creatinine = df$serum_creatinine,
  serum_sodium = df$serum_sodium,
  time = df$time
  )
color_list = list('darkgreen','blue','purple','lightblue', 'red')
n = 1
for (i in lst) {
  print(
    ggplot(df) + 
      geom_boxplot(aes(as.factor(DEATH_EVENT), i),fill = color_list[n],alpha = 0.5) +
      labs(
        x = 'DEATH_EVENT',
        y = names(lst[n])
      )
    )
      
    n = n + 1
}
```

Some insights we can make are:

-   Subjects who had a shorter returning appointment *time* were most likely to experience heart failure.

-   Subjects with a lower *ejection fraction* were most likely to experience heart failure.

-   Subjects with a higher *serum creatinine* level were more likely to experience heart failure.

-   Subjects with a lower *serum sodium* level were more likely to experience heart failure.

# Data Processing

Below we are turning our categorical features/response variables into factors. After that, we begin our train/test split into a 75% train size.

```{r}
cols = c(
  'anaemia', 'diabetes', 'high_blood_pressure',
  'sex', 'smoking', 'DEATH_EVENT'
  )

df <- df %>%
  mutate_at(cols, as.factor)

set.seed(123)
split <- initial_split(df, 0.75, strata = DEATH_EVENT)
train <- training(split)
test <- testing(split)

# kfold validation
set.seed(123)
kfold <- vfold_cv(train, v = 10, strata = DEATH_EVENT)
```

## Feature Engineering

Now, we are creating a recipe to normalize our predictors.

```{r}
# recipe
rcp <- recipe(DEATH_EVENT ~ ., data = train) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_YeoJohnson(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_other(all_nominal_predictors(), threshold = 0.05, other = 'other')

```

# Machine Learning

Below are 2 different supervised classification algorithms to help predict heart failure mortality:

-   Logistic Regression

-   Random Forest

We will create these two models and see which of these two algorithms are more efficient.

## Logistic Regression

### Tuning Model

```{r, warning=FALSE}
logit_model <- logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine('glmnet')

logit_grid <- grid_regular(
  penalty(range = c(-3, -1)),
  mixture(),
  levels = 10
  )

tuning <- workflow() %>%
  add_recipe(rcp) %>%
  add_model(logit_model) %>%
  tune_grid(resamples = kfold, grid = logit_grid, control = control_resamples(save_pred = T)) 

show_best(tuning, metric = 'roc_auc')
  
```

### ROC Curve

```{r}
collect_predictions(tuning) %>%
  group_by(id) %>%
  roc_curve(DEATH_EVENT, .pred_0) %>%
  autoplot()
```

### Finalizing Model

```{r}
best_hyperparameters_lg <- select_best(tuning, metric = 'roc_auc')

final_lg_wf <- workflow() %>%
  add_model(logit_model) %>%
  add_recipe(rcp) %>%
  finalize_workflow(best_hyperparameters_lg)

set.seed(123)
final_fit_lg <- final_lg_wf %>%
  fit(data = train)
  
final_test_lg <- final_fit_lg %>%
  predict(test) %>%
  bind_cols(select(test, DEATH_EVENT)) 

# metrics
logit_acc <- final_test_lg %>%
  accuracy(DEATH_EVENT, .pred_class)

final_test_lg %>%
  conf_mat(DEATH_EVENT, .pred_class) 
```

Our Logistic Regression model had a **85.3%** accuracy rate to new data (83.9% accuracy in our cross validation procedure). Now we will evaluate which features were the most important in predicting heart failure mortality classification in this model.

### Feature Importance

```{r}
final_fit_lg %>%
  extract_fit_parsnip() %>%
  vip() +
  ggtitle('Feature Importance') 
```

*Time* and *Serum_Creatinine* were the two most important features in our Logistic Regression model. Now we are going to look at the partial dependency graphs for these two variables.

First we have to create a function that creates these partial dependency plots.

```{r}
predict_plot <- function(var, model) {
  explainer_lg <- explain_tidymodels(
    model,
    data = select(train, -DEATH_EVENT),
    y = as.integer(train$DEATH_EVENT)
  )
  
  pdp <- model_profile(
    explainer = explainer_lg,
    variables = var,
    N = NULL
  )
  
  pdp_df <- as_tibble(pdp$agr_profiles)
  print(
  ggplot(pdp_df, aes(x = `_x_`, y = `_yhat_`)) +
    geom_smooth(color = 'lightblue', se = F) +
    ylim(0, 1) +
    labs(
      title = paste(str_to_title(var), 'Partial Prediction'),
      x = var
    ) +
    theme_dark() +
    theme(plot.title = element_text(hjust = 0.5))
  )
}

```

Below is the partial dependency graph for the *time* feature:

```{r, message=FALSE}
predict_plot(var = 'time', model = final_fit_lg)
```

As shown, the longer the time until the next appointment is, the lower chance the patient had heart failure. This could be due to the patient having a lower severity than patients who need to come back in shorter intervals.

Below is the partial dependency plot for the feature *serum_creatinine*:

```{r, message=FALSE}
predict_plot(var = 'serum_creatinine', model = final_fit_lg)
```

The probability of heart failure increases as serum creatinine levels increase, but increase at a slower rate at every increase in serum creatinine level.

## Random Forest

### Tuning Model

```{r, warning=FALSE}
rf_model <- rand_forest(
  mode = 'classification',
  trees = 500,
  mtry = tune(),
  min_n = tune()
  ) %>%
  set_engine("ranger", importance = "permutation")

hyper_grid <- grid_regular(
   mtry(range = c(2, 12)),
   min_n(range = c(1, 10)),        
   levels = 10
   )

# train our model across the hyper parameter grid
set.seed(123)
results <- tune_grid(
  rf_model, 
  rcp, 
  resamples = kfold, 
  grid = hyper_grid,
  control = control_resamples(save_pred = T) 
  )

# model results
show_best(results, metric = "roc_auc")

```

### ROC Curve

```{r}
collect_predictions(results) %>%
  group_by(id) %>%
  roc_curve(DEATH_EVENT, .pred_0) %>%
  autoplot()
```

### Finalizing Model

```{r}
best_hyperparameters <- select_best(results, metric = 'roc_auc')

final_rf_wf <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rcp) %>%
  finalize_workflow(best_hyperparameters)

final_fit_rf <- final_rf_wf %>%
  fit(data = train)
  
set.seed(123)
final_test_rf <- final_fit_rf %>%
  predict(test) %>%
  bind_cols(select(test, DEATH_EVENT)) 

# metrics
rf_acc <- final_test_rf %>%
  accuracy(DEATH_EVENT, .pred_class)

final_test_rf %>%
  conf_mat(DEATH_EVENT, .pred_class) 

```

Our Random Forest model predicts an **88%** accuracy to new data (84.8% accuracy in our cross validation set). Now lets look at which features were most important to determining mortality classification.

### Feature Importance

```{r}
final_fit_rf %>%
  extract_fit_parsnip() %>%
  vip() +
  ggtitle('Feature Importance')
```

We can see that *time* is by far the most important feature. Lets explore the partial dependency plot for this feature in our Random Forest Model.

```{r, message=FALSE}
predict_plot(var = 'time', model = final_fit_rf)
```

The plot shows the same overall relationship as our time predictor in our Logistic Regression model. However, there is a steeper slope down in probability but eventually gets less steep and levels out around 110 days.

# Results

```{r}
acc_df <- data.frame(
  algorithm = c('Logistic Regression', 'Random Forest'),
  accuracy = c(logit_acc$.estimate, rf_acc$.estimate)
) 

acc_percent <- paste(
  as.character(round(acc_df$accuracy, 3) * 100),
  '%'
)

acc_df %>%
  ggplot(aes(algorithm, accuracy)) +
    geom_segment(
      aes(
        xend = algorithm,
        x = algorithm,
        yend = accuracy,
        y = 0
        ),
      color = 'grey'
      ) +
    geom_point(size = 4, aes(color = algorithm)) +
  scale_color_manual(values = c("grey", "darkgreen")) +
  theme_classic() +
  coord_flip() +
  geom_text(label = acc_percent, nudge_y = 0.1) +
  labs(
    title = 'Model Accuracy'
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    legend.title = element_blank(),
    legend.text = element_blank(),
    legend.position = 'none',
    plot.title = element_text(hjust = 0.5, face = 'bold')
    ) 
```

As shown, our Random Forest Model **(88%)** has a higher accuracy than our Logistic Regression model **(85.3%)**. *Time* was the most important feature in determining mortality classification.
